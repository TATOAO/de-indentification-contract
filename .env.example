# NER 模型配置
# ==============

# 适配器类型：可选值 "modelscope"（推荐）、"paddlenlp"、"llm"
# 默认值: modelscope
NER_ADAPTER_TYPE=modelscope

# 模型名称（根据适配器类型不同而不同）
# ModelScope 示例: damo/nlp_structbert_named-entity-recognition_chinese-base-ecommerce
# PaddleNLP 示例: uie-base
# 如果不设置，将使用适配器的默认模型
NER_MODEL_NAME="iic/nlp_deberta_rex-uninlu_chinese-base"

# 本地模型路径（如果已下载模型到本地）
# 支持绝对路径和相对路径（相对于项目根目录）
# 如果不设置，将从 Hub 下载或使用缓存
NER_MODEL_PATH="./models"

# 实体类型列表（JSON 格式或逗号分隔）
# 默认值: ["组织机构", "人名", "地点"]
# 示例（JSON）: ["组织机构", "人名", "地点", "金额"]
# 示例（逗号分隔）: 组织机构,人名,地点
NER_SCHEMA=["组织机构", "人名", "地点"]

# ModelScope 缓存目录
# 如果不设置，将使用项目 models/modelscope 目录
# MODEL_SCOPE_CACHE_DIR=

# PaddleNLP 模型目录（PADDLE_HOME）
# 如果不设置，将使用项目 models 目录或默认缓存目录
# PADDLE_HOME=

# 模型存储目录（项目根目录下的 models 目录）
# 如果不设置，将使用项目根目录下的 models 目录
# MODELS_DIR=

# LLM 适配器配置
# ==============

# LLM 调用函数的模块路径（用于动态导入）
# 格式: module.path:function_name
# 例如: my_module.llm_utils:call_openai_api
# 如果不设置，LLM 适配器将尝试自动检测可用的后端
# LLM_CALL_FUNC_MODULE=
